---
title: 'Avatar: The Last Airbender Analysis'
author: "Nick Cruickshank"
date: "9/24/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Information

```{r load libraries, message=FALSE, warning=FALSE}
# load libraries
library(extrafont)
library(forcats)
library(glmnet)
library(glue)
library(janitor)
library(ngram)
library(readr)
library(shadowtext)
library(tidyverse)
library(tidymodels)
library(tvthemes)
loadfonts(device = "win")
```

```{r load data, message=FALSE, warning=FALSE}
# load data
avatar <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-08-11/avatar.csv')
scene_description <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-08-11/scene_description.csv')
```


```{r tidy df, message=FALSE, warning=FALSE}
# create ep list
eps <- avatar %>%
  distinct(book, book_num, chapter, chapter_num, writer, director, imdb_rating) %>%
  #mutate(writer = str_remove(writer, "<U+200E>")) %>%
  separate(writer, into = c("writer1","writer2","writer3","writer4","writer5",
                            "writer6","writer7","writer8","writer9","writer10"),
           sep = ", ")
# for each character, create a column with percent of spoke lines

books <- unique(avatar$book)
```

## Describe DF

```{r}
knitr::kable(avatar %>% head(1))
```

## Exploratory Analysis

### Episode Breakdown by IMDB Rating

```{r}
# worst episode
worst_ep <- eps %>%
  arrange(imdb_rating) %>%
  select(chapter, imdb_rating) %>%
  head(1)

worst_chap <- worst_ep$chapter
worst_chap_rating <- worst_ep$imdb_rating

# best episode
best_ep <- eps %>%
  arrange(desc(imdb_rating)) %>%
  select(chapter, imdb_rating) %>%
  head(1)

best_chap <- best_ep$chapter
best_chap_rating <- best_ep$imdb_rating
```


```{r episode breakdown, fig.height=8, fig.width=7, message=FALSE, warning=FALSE}
# create a graph visualizing the episodes by imbd_rating
eps %>%
  ggplot(aes(book_num, chapter_num)) + 
  geom_tile(aes(fill = imdb_rating), color = "black") + 
  scale_fill_viridis_c(option = "plasma") + 
  geom_shadowtext(aes(label = chapter), size = 2.5) + 
  labs(
    title = "Avatar Episode IMDb Ratings",
    subtitle = glue("Best episode was '{best_chap}' with a rating of {best_chap_rating}\n
                    Worst episode was '{worst_chap}' with a rating of {worst_chap_rating}"),
    x = "Book",
    y = "Chapter",
    fill = "IMBd Rating"
  ) +
  theme_avatar(title.font = "Herculanum",
               text.font = "Herculanum") + 
  theme(
    legend.position = c(0.2,0.96),
    legend.direction = "horizontal",
    legend.box.background = element_rect(),
    panel.grid = element_blank()
  )
```

### Director and Writer Analysis

Which directors or writers were associated with the most succesful episodes of Avatar?

### "Cabbages" Trending

Track the running joke for the "MY CABBAGES" joke.

### Zuko Transformation Analysis

## Which characters had the most lines?

For each line of each episode, create a column which counts the number of words (excluding stop words?) that line contains.

```{r create df for word count by character, message=FALSE, warning=FALSE}
# define strings to trim from dataset.
intro1 <- "Water. Earth. Fire. Air. My grandmother used to tell me"
intro2 <- "Long ago, the four nations lived together in harmony."

character_words <- avatar %>%
  filter(!(grepl(c(intro1,intro2), character_words)),
         character != "Scene Description") %>%
  group_by(book, book_num, chapter, chapter_num, character, writer, director, imdb_rating) %>%
  dplyr::summarise(
    words = wordcount(character_words)
  )
  #mutate(words = wordcount(character_words)) #doesn't work right, appears to work within a summarise
```

```{r word count distribution, message=FALSE, warning=FALSE}
main_characters <- c("Aang", "Azula", "Iroh", "Katara", "Ozai", "Sokka", "Toph", "Zuko")

#font_import(pattern = "herculanum.ttf", paths = "C:\\Windows\\Fonts\\", prompt = F)
#font_import(prompt = F)

character_words %>%
  filter(character %in% main_characters) %>%
  group_by(book, character) %>%
  dplyr::summarise(
    total_words = sum(words)
  ) %>%
  ggplot(aes(fct_reorder(character, total_words, .desc = TRUE), total_words)) + 
  geom_bar(aes(fill = book), stat = "identity", color = "black") + 
  scale_fill_manual(values = c(
    "Earth" = "tan4",
    "Fire" = "firebrick",
    "Water" = "royalblue2"
  )) +
  labs(
    title = "Avatar: Word Count By Character",
    x = "Character",
    y = "Total Words"
  ) +
  theme_avatar(title.font = "Herculanum",
               text.font = "Herculanum")
```


Pivot the dataframe wider, so each character gets a column whose values are the number of words they had in the episode.

Apply the same logic for each writer and director, but instead the column is boolean (i.e. If writer one is in s1e1, than the column value is 1 for that episode, else 0).

Train-test split the resulting dataframe, with IMDb rating as the y-value.

Assess numerous different models for accuracy.

## Machine Learning

Inspired by Julia Silge: https://juliasilge.com/blog/lasso-the-office/

```{r}
remove_regex <- "[:punct:]|[:digit:]|parts |parts |part |the |and" # be careful defining this

# create df to tidy up chapter names
avatar_info <- avatar %>%
  mutate(
    chap = str_to_lower(chapter),
    chap = str_remove_all(chap, remove_regex),
    chap = str_trim(chap),
    imdb_rating
  )

avatar_info
```

```{r}
avatar_ratings <- avatar_info %>%
  distinct(chap, imdb_rating)
```


How many times did each character speak per episode?

```{r}
characters <- avatar_info %>%
  count(chap, character) %>%
  add_count(character, wt = n, name = "character_count") %>%
  filter(character_count > 50,
         character != "Scene Description") %>%
  select(-character_count) %>%
  pivot_wider(
    names_from = character,
    values_from = n,
    values_fill = list(n = 0)
    )

characters
```

Which writers/directors were involved in each episode?

```{r}
creators <- avatar_info %>%
  distinct(chap, director, writer) %>%
  pivot_longer(director:writer, names_to = "role", values_to = "person") %>%
  separate_rows(person, sep = ",(\\s)?") %>%
  add_count(person) %>%
  filter(n > 1) %>%
  distinct(chap, person) %>%
  mutate(person_value = 1) %>%
  pivot_wider(
    names_from = person,
    values_from = person_value,
    values_fill = list(person_value = 0)
  )

creators
```

Join everything together

```{r}
df <- avatar_info %>%
  distinct(book, book_num, chap, chapter_num) %>%
  inner_join(characters) %>%
  inner_join(creators) %>%
  inner_join(avatar_ratings %>%
               select(chap, imdb_rating)) %>%
  janitor::clean_names()

df$imdb_rating[is.na(df$imdb_rating)] <- 9.4

df
```

Some brief exploratory data analysis (EDA)

```{r message=FALSE, warning=FALSE}
df %>%
  ggplot(aes(chapter_num, imdb_rating, fill = as.factor(chapter_num))) + 
  geom_boxplot(show.legend = FALSE) + 
  labs(
    title  = "Prelim Avatar EDA"
  ) + 
  theme_avatar(title.font = "Herculanum",
               text.font = "Herculanum")
```

## Training the model

Perform initial split.

```{r}
df_split <- initial_split(df, strata = book_num)
df_train <- training(df_split)
df_test <- training(df_split)
```

Preprocessing. Mainly setting index.

```{r}
df_rec <- recipe(imdb_rating ~ ., data = df_train) %>%
  update_role(book, new_role = "ID") %>%
  update_role(book_num, new_role = "ID") %>%
  update_role(chap, new_role = "ID") %>%
  update_role(chapter_num, new_role = "ID") %>%
  step_zv(all_predictors(), -all_outcomes()) %>%
  step_normalize(all_predictors(), -all_outcomes())

df_prep <- df_rec %>%
  prep(strings_as_factors = FALSE)

#recipe(imdb_rating ~ ., data = df_train) %>%
  #update_role(book, new_role = "ID") %>%
  #update_role(book_num, new_role = "ID") %>%
  #update_role(chap, new_role = "ID") %>%
  #update_role(chapter_num, new_role = "ID") %>%
  #step_zv(all_predictors(), -all_outcomes()) %>%
  #step_normalize(all_predictors(), -all_outcomes())
```

assign model (linear_reg lasso) and fit the training data to it.

```{r}
lasso <- linear_reg(penalty = 0.1, mixture = 1) %>%
  set_engine("glmnet")

wf <- workflow() %>%
  add_recipe(df_rec)

lasso_fit <- wf %>%
  add_model(lasso) %>%
  fit(data = df_train)

lasso_fit %>%
  pull_workflow_fit() %>%
  tidy()
```

tune the lasso model

define parameters to tune by

```{r}
set.seed(1494)
df_boot <- bootstraps(df_train, stata = book_num)

tune_spec <- linear_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet")

lambda_grid <- grid_regular(penalty(), levels = 50)
```

tune the grid using workflow objects

```{r}
doParallel::registerDoParallel()
```

```{r}
set.seed(1994)

# is this like GridSearchCV?
lasso_grid <- tune_grid(
  wf %>% add_model(tune_spec),
  resamples = df_boot,
  grid = lambda_grid
)
```

Here are the results

```{r}
lasso_grid %>%
  collect_metrics()
```

Visualize the results

```{r}
lasso_grid %>%
  collect_metrics() %>%
  ggplot(aes(penalty, mean, color = .metric)) + 
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean - std_err
  ),
  alpha = 0.5
  ) + 
  geom_line(size = 1.5) + 
  facet_wrap(~.metric, scales = "free", nrow = 2) + 
  scale_x_log10() + 
  theme(legend.position = "none")
```


